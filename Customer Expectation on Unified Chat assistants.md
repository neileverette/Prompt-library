# Cross-Product AI Assistant Consistency: UX Research Prompt

## 1. Research Focus

This research examines **customer expectations for AI assistant consistency across enterprise product portfolios**. When customers use multiple products from the same vendor (e.g., IBM's observability suite, Google Cloud's platform, Microsoft 365 apps), they expect assistants to provide:

1. **Consistent placement and triggering** - Assistants appear in the same location and are activated the same way
2. **Consistent visual design** - Assistants look and feel like the same product experience
3. **Consistent capabilities** - Assistants offer comparable functionality across products
4. **Cross-product integration** - Assistants can work across product boundaries to solve unified problems

This research will provide evidence supporting the hypothesis that **unified assistant experiences create net positive customer value** by reducing cognitive load, enabling cross-product workflows, and meeting customer expectations for consistency within a vendor's ecosystem.

## 2. Core Research Questions

- How do customers expect AI assistants to behave across different products from the same vendor?
- What pain points emerge when assistants vary in placement, design, or capability across a product suite?
- Which companies provide consistent assistant experiences across their portfolios, and which don't?
- What evidence exists that customers value (or are frustrated by the lack of) assistant consistency?
- How do unified assistants enable cross-product problem-solving that fragmented assistants cannot?

## 3. The "Car Controls" Analogy

Just as drivers expect the steering wheel, gas pedal, and brake to be in the same position regardless of which car they drive, **customers expect assistant experiences to be consistent regardless of which product they're using within a vendor's suite**.

Current state at IBM: Using Instana, then switching to Concert, then opening Turbonomic presents three completely different assistant experiences - different trigger locations, different visual designs, different capabilities, and no ability to work across products.

## 4. Predicted Customer Questions & Pain Points

When encountering inconsistent assistant experiences across a product portfolio, customers are likely to express frustration through questions like these:

### Placement & Triggering Confusion
- *"Why does the assistant icon move around the page when I switch between IBM tools?"*
- *"I got used to clicking the bottom-right corner in Instana, but now I can't find the assistant in Concert—where did it go?"*
- *"Do I need to learn a different way to open the assistant in every tool?"*

### Capability Gaps & Inconsistencies
- *"I can ask the assistant to analyze my logs in Product A, but when I switch to Product B, that same question doesn't work. Why?"*
- *"The assistant in Instana helped me troubleshoot application issues, but in Turbonomic it only seems to answer documentation questions. Shouldn't they both do the same things?"*
- *"I expected the assistant to have the same features across all your products—why is one more powerful than the others?"*
- *"Product A's assistant can create tickets and trigger actions, but Product B's assistant is just a glorified search box. Is this intentional?"*

### Cross-Product Workflow Frustrations
- *"I'm investigating an incident that spans Instana and Turbonomic—why can't the assistant help me connect the dots between both tools?"*
- *"I asked the assistant in Concert to pull data from Instana, and it said it can't access that. Aren't these supposed to work together?"*
- *"Why do I have to manually switch between three different assistants to solve one problem? Can't they talk to each other?"*
- *"I expected the assistant to work across all IBM products seamlessly, but it feels like I'm using completely separate tools."*

### Competitive Comparisons
- *"Datadog's assistant works the same way across their entire platform—why doesn't IBM do this?"*
- *"Microsoft's Copilot looks and works the same whether I'm in Excel, Teams, or PowerPoint. Why can't IBM's assistant be consistent like that?"*
- *"We're evaluating other vendors, and they all seem to have unified assistant experiences. What makes IBM different?"*

### General Expectation Mismatches
- *"I assumed the assistant would be the same across all IBM tools—isn't that how this should work?"*
- *"Why does it feel like each product team built their own assistant without talking to each other?"*
- *"I have to retrain my team every time we adopt a new IBM tool because the assistant works differently. That can't be right."*
- *"If these are all IBM products, why don't they feel like they're from the same company?"*

## 5. Research Analysis Framework

For each company/product portfolio analyzed, document:

### A. Portfolio Context
- Company name
- Product suite being analyzed (e.g., "IBM Automation & Observability Portfolio")
- Specific products examined within the suite
- Customer segment using these products together

### B. Assistant Consistency Analysis
- **Trigger Location**: Where/how is the assistant activated? (e.g., lower-right icon, upper-right menu, command palette)
- **Visual Design**: Does the assistant UI look consistent across products?
- **Capability Parity**: Do assistants offer comparable features across products, or do some products have limited assistants?
- **Cross-Product Integration**: Can the assistant in Product A access data or take actions in Product B?

### C. Customer Experience Assessment
- What would a customer switching between products notice?
- What cognitive load is created by inconsistency?
- What workflows are enabled or blocked by the current design?
- Evidence of customer feedback (reviews, forum posts, support tickets mentioning assistant confusion)
- **Which predicted customer questions from Section 4 are validated by real evidence?**

## 6. Screenshot Reference Table

Screenshots will illustrate assistant experiences across different products within the same vendor's portfolio.

| Figure | Token | Expected Content |
|--------|-------------------------------|--------------------------------------------|
| Figure 1 | screenshot_1 | Product A assistant experience |
| Figure 2 | screenshot_2 | Product B assistant experience |
| Figure 3 | screenshot_3 | Product C assistant experience |
| Figure 4 | screenshot_4 | Assistant trigger/placement comparison |
| Figure 5 | screenshot_5 | Visual design comparison |
| Figure 6 | screenshot_6 | Capability comparison |
| Figure 7 | screenshot_7 | Cross-product workflow example (or lack thereof) |
| Figure 8 | screenshot_8 | Customer feedback or pain point evidence |
| Figure 9 | screenshot_9 | Best-in-class unified assistant example |

## 7. Screenshot Usage Instructions

When analyzing, always reference both the **Figure number** and the **token** to illustrate consistency (or inconsistency) patterns.

Example:

> "Figure 2 (screenshot_2) shows Product B's assistant triggered from the upper-right corner, while Figure 1 (screenshot_1) shows Product A's assistant in the lower-right—requiring customers to learn different interaction patterns for tools they use together daily. This validates the customer question: *'Why does the assistant icon move around the page when I switch between IBM tools?'*"

## 8. Output Requirements

Your analysis must include:

### 1. **Portfolio Consistency Matrix**
A comparison table showing:
- Company/Portfolio
- Products Analyzed
- Trigger Location Consistency (Yes/Partial/No)
- Visual Design Consistency (Yes/Partial/No)
- Capability Parity (Yes/Partial/No)
- Cross-Product Integration (Yes/No)
- Overall Consistency Score

### 2. **Customer Expectation Analysis**
Evidence and reasoning for why customers expect consistency, including:
- Analogies to other product experiences (cars, operating systems, etc.)
- Cognitive load research supporting consistency benefits
- Customer feedback examples from forums, reviews, or support channels
- **Validation of predicted customer questions from Section 4 with real-world evidence**

### 3. **Pain Point Catalog**
Document specific customer pain points caused by inconsistent assistants:
- Learning multiple interaction patterns
- Uncertainty about capability differences
- Inability to solve cross-product problems
- Frustration when switching contexts
- **Map pain points to the predicted customer questions in Section 4**

### 4. **Best Practice Examples**
Identify companies that DO provide consistent assistant experiences across their portfolios, documenting:
- What they do well
- How they maintain consistency
- Evidence of positive customer reception
- **How they avoid the pain points expressed in predicted customer questions**

### 5. **The Business Case for Consistency**
Summarize why unified assistants create net positive value:
- Reduced friction for customers using multiple products
- Enablement of cross-product workflows and problem-solving
- Competitive differentiation through superior experience
- Reduced support burden from consistency
- **Quantifiable impact of addressing the customer questions/pain points in Section 4**

### 6. **Design Principles for Unified Assistants**
Articulate guiding principles for maintaining consistency across a product portfolio:
- Placement and triggering standards
- Visual design system requirements
- Capability baseline expectations
- Cross-product integration patterns
- **Principles that directly address the predicted customer questions in Section 4**

---

## 9. Focus Reminder

**This research is about customer expectations and experience, NOT technical implementation.** The goal is to build a compelling argument with supporting evidence that:

> Customers using multiple products from the same vendor expect AI assistants to work consistently across those products—with the same placement, appearance, capabilities, and ability to work across product boundaries. Fragmented assistant experiences create friction, confusion, and missed opportunities for cross-product problem-solving.

**The predicted customer questions in Section 4 serve as a lens for evaluating assistant consistency.** Real customer evidence that validates these questions strengthens the case for unified assistant experiences.

Technical implementation challenges are acknowledged but not the focus—this is about establishing what customers expect and why meeting those expectations matters.