# Cross-Product AI Assistant Consistency: UX Research Prompt

## 1. Research Focus

This research examines **customer expectations for AI assistant consistency across enterprise product portfolios**. When customers use multiple products from the same vendor (e.g., IBM's observability suite, Google Cloud's platform, Microsoft 365 apps), they expect assistants to provide:

1. **Consistent placement and triggering** - Assistants appear in the same location and are activated the same way
2. **Consistent visual design** - Assistants look and feel like the same product experience
3. **Consistent capabilities** - Assistants offer comparable functionality across products
4. **Cross-product integration** - Assistants can work across product boundaries to solve unified problems

This research will provide evidence supporting the hypothesis that **unified assistant experiences create net positive customer value** by reducing cognitive load, enabling cross-product workflows, and meeting customer expectations for consistency within a vendor's ecosystem.

## 2. Core Research Questions

- How do customers expect AI assistants to behave across different products from the same vendor?
- What pain points emerge when assistants vary in placement, design, or capability across a product suite?
- Which companies provide consistent assistant experiences across their portfolios, and which don't?
- What evidence exists that customers value (or are frustrated by the lack of) assistant consistency?
- How do unified assistants enable cross-product problem-solving that fragmented assistants cannot?

## 3. The "Car Controls" Analogy

Just as drivers expect the steering wheel, gas pedal, and brake to be in the same position regardless of which car they drive, **customers expect assistant experiences to be consistent regardless of which product they're using within a vendor's suite**.

Current state at IBM: Using Instana, then switching to Concert, then opening Turbonomic presents three completely different assistant experiences - different trigger locations, different visual designs, different capabilities, and no ability to work across products.

## 4. Predicted Customer Questions & Pain Points

When encountering inconsistent assistant experiences across a product portfolio, customers are likely to express frustration through questions like these:

### Placement & Triggering Confusion
- *"Why does the assistant icon move around the page when I switch between IBM tools?"*
- *"I got used to clicking the bottom-right corner in Instana, but now I can't find the assistant in Concert—where did it go?"*
- *"Do I need to learn a different way to open the assistant in every tool?"*

### Capability Gaps & Inconsistencies
- *"I can ask the assistant to analyze my logs in Product A, but when I switch to Product B, that same question doesn't work. Why?"*
- *"The assistant in Instana helped me troubleshoot application issues, but in Turbonomic it only seems to answer documentation questions. Shouldn't they both do the same things?"*
- *"I expected the assistant to have the same features across all your products—why is one more powerful than the others?"*
- *"Product A's assistant can create tickets and trigger actions, but Product B's assistant is just a glorified search box. Is this intentional?"*

### Cross-Product Workflow Frustrations
- *"I'm investigating an incident that spans Instana and Turbonomic—why can't the assistant help me connect the dots between both tools?"*
- *"I asked the assistant in Concert to pull data from Instana, and it said it can't access that. Aren't these supposed to work together?"*
- *"Why do I have to manually switch between three different assistants to solve one problem? Can't they talk to each other?"*
- *"I expected the assistant to work across all IBM products seamlessly, but it feels like I'm using completely separate tools."*

### Competitive Comparisons
- *"Datadog's assistant works the same way across their entire platform—why doesn't IBM do this?"*
- *"Microsoft's Copilot looks and works the same whether I'm in Excel, Teams, or PowerPoint. Why can't IBM's assistant be consistent like that?"*
- *"We're evaluating other vendors, and they all seem to have unified assistant experiences. What makes IBM different?"*

### General Expectation Mismatches
- *"I assumed the assistant would be the same across all IBM tools—isn't that how this should work?"*
- *"Why does it feel like each product team built their own assistant without talking to each other?"*
- *"I have to retrain my team every time we adopt a new IBM tool because the assistant works differently. That can't be right."*
- *"If these are all IBM products, why don't they feel like they're from the same company?"*

## 5. Research Analysis Framework

For each company/product portfolio analyzed, document:

### A. Portfolio Context
- Company name
- Product suite being analyzed (e.g., "IBM Automation & Observability Portfolio")
- Specific products examined within the suite
- Customer segment using these products together

### B. Assistant Consistency Analysis
- **Trigger Location**: Where/how is the assistant activated? (e.g., lower-right icon, upper-right menu, command palette)
- **Visual Design**: Does the assistant UI look consistent across products?
- **Capability Parity**: Do assistants offer comparable features across products, or do some products have limited assistants?
- **Cross-Product Integration**: Can the assistant in Product A access data or take actions in Product B?

### C. Customer Experience Assessment
- What would a customer switching between products notice?
- What cognitive load is created by inconsistency?
- What workflows are enabled or blocked by the current design?
- Evidence of customer feedback (reviews, forum posts, support tickets mentioning assistant confusion)
- **Which predicted customer questions from Section 4 are validated by real evidence?**

## 6. Screenshot Reference Table

Screenshots will illustrate assistant experiences across different products within the same vendor's portfolio.

| Figure | Token | Expected Content |
|--------|-------------------------------|--------------------------------------------|
| Figure 1 | screenshot_1 | Product A assistant experience |
| Figure 2 | screenshot_2 | Product B assistant experience |
| Figure 3 | screenshot_3 | Product C assistant experience |
| Figure 4 | screenshot_4 | Assistant trigger/placement comparison |
| Figure 5 | screenshot_5 | Visual design comparison |
| Figure 6 | screenshot_6 | Capability comparison |
| Figure 7 | screenshot_7 | Cross-product workflow example (or lack thereof) |
| Figure 8 | screenshot_8 | Customer feedback or pain point evidence |
| Figure 9 | screenshot_9 | Best-in-class unified assistant example |

## 7. Screenshot Usage Instructions

When analyzing, always reference both the **Figure number** and the **token** to illustrate consistency (or inconsistency) patterns.

Example:

> "Figure 2 (screenshot_2) shows Product B's assistant triggered from the upper-right corner, while Figure 1 (screenshot_1) shows Product A's assistant in the lower-right—requiring customers to learn different interaction patterns for tools they use together daily. This validates the customer question: *'Why does the assistant icon move around the page when I switch between IBM tools?'*"

## 8. Output Requirements

Your analysis must include:

### 1. **Portfolio Consistency Matrix**
A comparison table showing:
- Company/Portfolio
- Products Analyzed
- Trigger Location Consistency (Yes/Partial/No)
- Visual Design Consistency (Yes/Partial/No)
- Capability Parity (Yes/Partial/No)
- Cross-Product Integration (Yes/No)
- Overall Consistency Score

### 2. **Customer Expectation Analysis**
Evidence and reasoning for why customers expect consistency, including:
- Analogies to other product experiences (cars, operating systems, etc.)
- Cognitive load research supporting consistency benefits
- Customer feedback examples from forums, reviews, or support channels
- **Validation of predicted customer questions from Section 4 with real-world evidence**

### 3. **Pain Point Catalog**
Document specific customer pain points caused by inconsistent assistants:
- Learning multiple interaction patterns
- Uncertainty about capability differences
- Inability to solve cross-product problems
- Frustration when switching contexts
- **Map pain points to the predicted customer questions in Section 4**

### 4. **Best Practice Examples**
Identify companies that DO provide consistent assistant experiences across their portfolios, documenting:
- What they do well
- How they maintain consistency
- Evidence of positive customer reception
- **How they avoid the pain points expressed in predicted customer questions**

### 5. **The Business Case for Consistency**
Summarize why unified assistants create net positive value:
- Reduced friction for customers using multiple products
- Enablement of cross-product workflows and problem-solving
- Competitive differentiation through superior experience
- Reduced support burden from consistency
- **Quantifiable impact of addressing the customer questions/pain points in Section 4**

### 6. **Design Principles for Unified Assistants**
Articulate guiding principles for maintaining consistency across a product portfolio:
- Placement and triggering standards
- Visual design system requirements
- Capability baseline expectations
- Cross-product integration patterns
- **Principles that directly address the predicted customer questions in Section 4**

---

## 9. Focus Reminder

**This research is about customer expectations and experience, NOT technical implementation.** The goal is to build a compelling argument with supporting evidence that:

> Customers using multiple products from the same vendor expect AI assistants to work consistently across those products—with the same placement, appearance, capabilities, and ability to work across product boundaries. Fragmented assistant experiences create friction, confusion, and missed opportunities for cross-product problem-solving.

**The predicted customer questions in Section 4 serve as a lens for evaluating assistant consistency.** Real customer evidence that validates these questions strengthens the case for unified assistant experiences.

Technical implementation challenges are acknowledged but not the focus—this is about establishing what customers expect and why meeting those expectations matters.


## **10. Cross-Role Review & Perspective Questions**

At the conclusion of this research, the findings are reviewed through **two additional lenses** to ensure balance between strategy and experience. These perspectives intentionally challenge and complement the Product Researcher’s synthesis.

This section uses the **role definitions provided below** to generate structured questions that surface blind spots, assumptions, and implications.

---

### **A. Product Manager Perspective**

Using the **Product Manager** role definition, the following questions are raised after reviewing the research:

* What insights here materially change prioritization or roadmap decisions?
* Which inconsistencies represent the highest business risk if left unaddressed?
* Where does customer pain intersect most strongly with revenue, retention, or expansion?
* Are we solving a portfolio-level problem or optimizing individual products?
* What evidence is strong enough to justify cross-product alignment investment?
* What assumptions need validation before committing resources?
* If we could only fix one inconsistency, which delivers the highest impact?

**Goal:** Pressure-test the research for strategic relevance, business value, and feasibility.

---

### **B. Product Designer Perspective**

Using the **Product Designer** role definition, the following questions are raised after reviewing the research:

* Where does inconsistency most increase cognitive load for users?
* Which moments in the workflow feel most fragmented when switching products?
* What patterns appear consistent enough to become shared design standards?
* What interaction or placement decisions most undermine learnability?
* Where could a unified assistant materially simplify the user journey?
* What concepts need visualization to clarify the problem?
* Which insights should directly inform layout, flow, or component decisions?

**Goal:** Translate synthesized insights into actionable experience and interaction guidance.

---

### **Outcome of Step 10**

This step ensures the research does not remain descriptive, but becomes **directional**:

* Aligns insights with **product strategy**
* Translates findings into **experience implications**
* Surfaces tension between desirability, viability, and usability
* Creates a shared understanding across roles

The final output of the research includes:
* Synthesized insights (from the Product Researcher)
* Strategic challenges (from the Product Manager)
* Experience-driven questions (from the Product Designer)

Together, these perspectives ensure the research meaningfully informs decision-making across disciplines.  



---
### Profiles

## **Product Researcher (Secondary Research)**

### **Description**

Conducts in-depth secondary product research to understand customer experience, market dynamics, competitive offerings, and technical approaches. Investigates disparate sources, normalizes inconsistent data, and synthesizes insights into clear, comparable frameworks that inform product and go-to-market decisions.

### **Skills & Knowledge**

* Secondary research and investigation techniques  
* Competitive and market analysis  
* Data aggregation and normalization across uneven sources  
* Pattern recognition and synthesis  
* Translating complex inputs into comparable models  
* Strong inquisitive and critical thinking mindset  

### **Expected Output**

* Aggregated research summaries  
* Normalized comparison frameworks (features, CX, GTM, technical)  
* Clear synthesis of common themes and gaps  
* Distilled core insights and implications  
* Structured narratives to support product strategy and decision-making  

> **Opinion:** This role is most impactful when positioned as an *insight distiller*, not just a researcher — the value comes from reducing complexity and enabling faster, clearer decisions.


--

## **Product Designer**

### **Description**

Designs conceptual product experiences using HTML-structured wireframes, interaction flows, and layout explorations. Converts requirements and research into visual and structural UI concepts.

### **Skills & Knowledge**

* Wireframing and interaction design
* HTML-based structural representation
* User flows and story mapping
* Designing for technical and data-dense UIs
* Understanding cognitive load in incident workflows

### **Expected Output**

* Provides feedback
* Asks clarifying questions for understanding
* HTML-based wireframes
* Interaction flows
* Component placement (Insight Card, Deployment Trace, Topology)
* Rationale for layout decisions

---

## **9. Product Manager**

### **Description**

Defines strategic context, business value, feasibility considerations, and prioritization. Evaluates concepts for user value, differentiation, alignment with business goals, and engineering constraints.

### **Skills & Knowledge**

* Market analysis and competitive awareness
* Product strategy and roadmap thinking
* Strong communication across engineering and design
* Business case evaluation
* Understanding of technical feasibility constraints
* Set tone to be slightly skeptical
* Provides constructive feedback to guide the research

### **Expected Output**

* Strategic feedback
* Prioritization guidance
* Business-value assessments
* Short, actionable recommendations for UX or engineering
